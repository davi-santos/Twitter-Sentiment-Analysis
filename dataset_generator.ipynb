{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de sentimentos do Twitter e bolsa de valores: uma investigação sobre a correlação entre estes dois sistemas\n",
    "\n",
    "     Este projeto tem por objetivo investigar se existe correlação entre a opinião pública sobre os papéis da bolsa de valores na rede social Twitter e o retorno das ações na bolsa de valores. Para isto, aplicar-se-á técnicas de processamento de linguagem natural para extração de sentimentos, a análise de sentimentos, e algoritmos de machine learning a fim de averiguar se há tal correlação. Os códigos-fonte foram divididos em 3 arquivos: Geração de dados, análise dos dados e aplicação de machine learning.\n",
    "\n",
    "## Arquivo 1: Gerador de dados\n",
    "\n",
    "Este módulo tem por objetivo:\n",
    "\n",
    "    1. Gerar uma base de dados e salvar em arquivo local;\n",
    "    2. Persistir a base de dados no MongoDB;\n",
    "\n",
    "### 1. Preparando Ambiente\n",
    "\n",
    "#### 1.1 Instalando dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#yahoo finances para recuperar dados da bolsa de valores\n",
    "!pip install yfinance --upgrade --no-cache-dir\n",
    "\n",
    "#Scrape tweet ids\n",
    "!pip install snscrape\n",
    "\n",
    "#API to retrieve tweets by id\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Importando bibliotecas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#API Yahoo Finance\n",
    "import yfinance as yf\n",
    "#API twitter\n",
    "import tweepy\n",
    "#Maniputalacao de dados em dataframe\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Aquisição da base de dados\n",
    "\n",
    "#### 2.1 Finanças\n",
    "Nesta etapa, baixa-se os dados financeiros através do Yahoo Finances e salva-os em uma lista de objetos do tipo dicionário do python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n",
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "#setting variable dates\n",
    "date_start = '2019-01-01'; date_end = '2019-05-31'\n",
    "\n",
    "# set directory to storage financial files\n",
    "path= './database/finance/'\n",
    "\n",
    "# Stock market tickers for downloading\n",
    "dict_search = {\n",
    "        'Magazine Luisa SA': 'MGLU3.SA',\n",
    "        'Petrobras': 'PETR4.SA',\n",
    "        'B3': 'B3SA3.SA', \n",
    "        'IRB Brasil Seguros': 'IRBR3.SA',\n",
    "}\n",
    "\n",
    "for key in dict_search:\n",
    "    df = yf.download(dict_search[key], start=date_start, end=date_end)\n",
    "    \n",
    "    #Saving to dataframe in local storage path\n",
    "    pd.DataFrame(df).to_csv(path+key+'.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Twitter\n",
    "\n",
    "#### snscrape\n",
    "\n",
    "O comando snscrape funciona, em geral, no prompt de comando:\n",
    "\n",
    "       snscrape twitter-search \"MGLU3 since:2019-01-01 until:2019-05-31\" > MGLU3_tweets.txt\n",
    "tal que, \"MGLU3\" foi o ticker buscado, \"since:2019-01-01\" e \"until:2019-05-31\" definem datas de início e término da busca. Assim, vamos utilizar este comando usando a biblioteca OS para fazer a busca das palavras-chave do twitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_terms = ['MGLU3', 'PETR4', 'B3SA3', 'IRBR3']\n",
    "file_names = ['Magalu', 'Petrobras','B3', 'IRB_Brasil_Seguros']\n",
    "base = 'snscrape twitter-search \"'\n",
    "since = '2019-01-01'\n",
    "until = '2019-05-31'\n",
    "\n",
    "for i,ticker in enumerate(search_terms):\n",
    "    os.system(base+ticker+' --since:'+since+' --until:'+until+'\" > ./snscrape/'+file_names[i]+\".txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora temos os arquivos necessários que contém os links dos tweets buscados. Agora, segue a etapa de recuperação dos tweets por meio do tweepy.\n",
    "\n",
    "#### Tweepy\n",
    "\n",
    "Conexão com a API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill the variables with your Twitter developer account credentials \n",
    "api_key = ''\n",
    "api_secret_key = ''\n",
    "access_token = ''\n",
    "access_token_secret = ''\n",
    "\n",
    "# Connecting to API\n",
    "auth = tweepy.OAuthHandler(api_key, api_secret_key) \n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definindo função de busca. Passa-se uma lista de ids e retorna uma lista de objetos do tipo dict python com os tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set directory to storage twitter files\n",
    "twitter_path = './database/twitter/'\n",
    "\n",
    "search_terms = ['MGLU3', 'PETR4', 'B3SA3', 'IRBR3']\n",
    "file_names = ['Magalu', 'Petrobras','B3', 'IRB_Brasil_Seguros']\n",
    "\n",
    "all_tweets = []\n",
    "\n",
    "def fetch_tw(ids):\n",
    "    list_of_tw_status = api.statuses_lookup(ids, tweet_mode= \"extended\")\n",
    "    empty_data_list = []\n",
    "    for status in list_of_tw_status:\n",
    "            tweet_elem = {\"date\":status.created_at,\n",
    "                          \"arroba\": status.user.screen_name,\n",
    "                          \"text\":status.full_text,\n",
    "                          \"tweet_id\": status.id,\n",
    "                          \"favorite\":status.favorite_count,\n",
    "                          \"retweet\": status.retweet_count\n",
    "                     }\n",
    "            empty_data_list.append(tweet_elem)\n",
    "    return empty_data_list\n",
    "\n",
    "for index, item in enumerate(search_terms):\n",
    "    \n",
    "    #Leitura dos scrapings\n",
    "    file_path = './snscrape/'+file_names[index]+'.txt'\n",
    "    tweet_url = pd.read_csv(file_path, index_col= None, header = None, names = [\"links\"])\n",
    "\n",
    "    #Extraindo id dos links\n",
    "    af = lambda x: x[\"links\"].split(\"/\")[-1]\n",
    "    tweet_url['id'] = tweet_url.apply(af, axis=1)\n",
    "    ids = tweet_url['id'].tolist()\n",
    "    \n",
    "    #Baixar tweets de 50 em 50\n",
    "    total_count = len(ids)\n",
    "    chunks = (total_count - 1) // 50 + 1\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for i in range(chunks):\n",
    "        batch = ids[i*50:(i+1)*50]\n",
    "        result += fetch_tw(batch)\n",
    "    \n",
    "    #Salvando localmente\n",
    "    #print(result)\n",
    "    df = pd.DataFrame(result)\n",
    "    df.to_csv(twitter_path+file_names[index]+'.csv')\n",
    "    \n",
    "    #salvar todos os tweets\n",
    "    all_tweets+= result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Observações durante as tarefas\n",
    "\n",
    "#### API Obsoleta GetOldTweets3\n",
    "\n",
    "Infelizmente, a API GetOldTweets3 já não funciona mais para buscas avançadas. Na tentativa de busca o servidor do Twitter reporta um erro e devolve um link. O link resposta funciona corretamente, mas o servidor não devolve os tweets requeridos. Isto deve-se a uma atualização do mecanismo de busca avançada dentro do Twitter que não existe mais e foi removida (/i/search/timeline) e que era utilizada pela API GetOldTweets.\n",
    "\n",
    "FONTE: https://medium.com/@jcldinco/downloading-historical-tweets-using-tweet-ids-via-snscrape-and-tweepy-5f4ecbf19032"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
